{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoRRSJmEcwTUT1Wa3Kh7Vw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FerbeiPatricia/Retele-neuronale2024/blob/main/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5nHUrCFwaGu",
        "outputId": "4cce88d2-3439-44b5-e441-a9415d22efcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1/20, Loss: 0.3330\n",
            "Epoca 2/20, Loss: 0.2674\n",
            "Epoca 3/20, Loss: 0.2228\n",
            "Epoca 4/20, Loss: 0.1900\n",
            "Epoca 5/20, Loss: 0.1690\n",
            "Epoca 6/20, Loss: 0.1518\n",
            "Epoca 7/20, Loss: 0.1418\n",
            "Epoca 8/20, Loss: 0.1297\n",
            "Epoca 9/20, Loss: 0.1228\n",
            "Epoca 10/20, Loss: 0.1193\n",
            "Epoca 11/20, Loss: 0.1179\n",
            "Epoca 12/20, Loss: 0.1085\n",
            "Epoca 13/20, Loss: 0.1066\n",
            "Epoca 14/20, Loss: 0.1007\n",
            "Epoca 15/20, Loss: 0.1004\n",
            "Epoca 16/20, Loss: 0.0947\n",
            "Epoca 17/20, Loss: 0.0932\n",
            "Epoca 18/20, Loss: 0.0930\n",
            "Epoca 19/20, Loss: 0.0951\n",
            "Epoca 20/20, Loss: 0.0895\n",
            "Acuratețea pe setul de testare : 0.9530\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    m = y_true.shape[0]\n",
        "    loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n",
        "    return loss\n",
        "\n",
        "def forward_propagation(X, W1, b1, W2, b2, dropout_rate=0.2, training=True):\n",
        "\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "\n",
        "    if training:\n",
        "        dropout_mask = (np.random.rand(*A1.shape) > dropout_rate).astype(float)\n",
        "        A1 *= dropout_mask\n",
        "        A1 /= (1 - dropout_rate)\n",
        "\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    y_pred = softmax(Z2)\n",
        "    return Z1, A1, Z2, y_pred\n",
        "\n",
        "\n",
        "def backpropagation(X, y_true, Z1, A1, Z2, y_pred, W1, b1, W2, b2, learning_rate):\n",
        "    m = y_true.shape[0]\n",
        "\n",
        "    #Z2 = A1 * W2 + b2\n",
        "    dZ2 = y_pred - y_true\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0) / m\n",
        "\n",
        "    #Z1 = X * W1 + b1\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0) / m\n",
        "\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def train_neural_network(train_X, train_Y, num_epochs, learning_rate, batch_size=100, dropout_rate=0.5):\n",
        "\n",
        "    W1 = np.random.randn(train_X.shape[1], 100) * 0.01\n",
        "    b1 = np.zeros((100,))\n",
        "    W2 = np.random.randn(100, 10) * 0.01\n",
        "    b2 = np.zeros((10,))\n",
        "\n",
        "    num_examples = train_X.shape[0]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        indices = np.arange(num_examples)\n",
        "        np.random.shuffle(indices)\n",
        "        train_X = train_X[indices]\n",
        "        train_Y = train_Y[indices]\n",
        "\n",
        "        for i in range(0, num_examples, batch_size):\n",
        "            X_batch = train_X[i:i + batch_size]\n",
        "            Y_batch = train_Y[i:i + batch_size]\n",
        "\n",
        "            Z1, A1, Z2, y_pred_batch = forward_propagation(X_batch, W1, b1, W2, b2, dropout_rate=dropout_rate, training=True)\n",
        "\n",
        "            W1, b1, W2, b2 = backpropagation(X_batch, Y_batch, Z1, A1, Z2, y_pred_batch, W1, b1, W2, b2, learning_rate)\n",
        "\n",
        "        _, _, _, y_pred_train = forward_propagation(train_X, W1, b1, W2, b2, dropout_rate=dropout_rate, training=False)\n",
        "        loss = cross_entropy_loss(y_pred_train, train_Y)\n",
        "        print(f\"Epoca {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def download_mnist(is_train: bool):\n",
        "    dataset = MNIST(root='./data', transform=lambda x: np.array(x).flatten(), download=True, train=is_train)\n",
        "    mnist_data = []\n",
        "    mnist_labels = []\n",
        "    for image, label in dataset:\n",
        "        mnist_data.append(image / 255.0)\n",
        "        mnist_labels.append(label)\n",
        "    mnist_data = np.array(mnist_data)\n",
        "    mnist_labels = np.array(mnist_labels)\n",
        "    mnist_labels_one_hot = np.eye(10)[mnist_labels]\n",
        "    return mnist_data, mnist_labels_one_hot\n",
        "\n",
        "train_X, train_Y = download_mnist(True)\n",
        "test_X, test_Y = download_mnist(False)\n",
        "\n",
        "\n",
        "num_epochs = 20\n",
        "learning_rate = 0.1\n",
        "W1, b1, W2, b2 = train_neural_network(train_X, train_Y, num_epochs, learning_rate, batch_size=100, dropout_rate=0.2)\n",
        "\n",
        "\n",
        "def test_predict(X, W1, b1, W2, b2):\n",
        "    _, _, _, y_pred = forward_propagation(X, W1, b1, W2, b2)\n",
        "    return np.argmax(y_pred, axis=1)\n",
        "\n",
        "y_test_pred = test_predict(test_X, W1, b1, W2, b2)\n",
        "test_accuracy = np.mean(np.argmax(test_Y, axis=1) == y_test_pred)\n",
        "print(f'Acuratețea pe setul de testare : {test_accuracy:.4f}')"
      ]
    }
  ]
}